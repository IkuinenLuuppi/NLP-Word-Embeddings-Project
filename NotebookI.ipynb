{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Word Embeddings\n",
    "\n",
    "\n",
    "The groupwork of the group number two.\n",
    "\n",
    "Source of truth: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course\n",
    "\n",
    "\n",
    "###### The group\n",
    "\n",
    "- Mikael Laine\n",
    "\n",
    "- Naren \"Puri\" Purighalla\n",
    "\n",
    "- Juho Kaasalainen\n",
    "\n",
    "- Mari Salonen\n",
    "\n",
    "###### The task\n",
    "\n",
    "Do stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrections/comments?" 
    "What points should I add/modify?" 
    "Remarks on the style (too formal)?" 
    "I expect brutal honesty (as always)." 
    "The text is quite short. I’ll write more when I understand more and when “we” have progressed."

     "#Word embeddings project"
     "The aim of this project is to train and test different word2vec models and further evaluate and compare them with each other. An existing word2vec model by Tomas Mikolov is used as a base from which different word2vec models are created by varying the parameters, such as the size of the word context window. "
     "The evaluation of these models is done first manually by checking the nearest neighbors of selected words. Then the models are tested and evaluated formally on existing web based evaluation scripts (?) and finally ranked according to their accuracy."
     "#Preparing the data"
     "As data, we decided to use the IMDB and the Reuters datasets, which are practical since they were already read in (?) and used during the lectures. In order to get the files in a usable form, the data is first shuffled to avoid distortion of the results and then tokenized. In the tokenization, which includes preprocessing (i.e., eliminating the punctuation, etc.), we use sklearn and numpy libraries."
     "#Parameters"
     "After the data is modified to the desired form, the training parameters are set using Mikolov’s word2vec model’s default settings as a starting point. Some parameters are left untouched, for example the initialization of the weights is left to its original state: alpha (?), whereas three parameters are changed: Window sizes 3, 5, 7 and 9 are chosen to allow us to examine the effect of the word context on the embeddings. As for the architecture, we use the Continuous Bag of Words (CBOW) model, which learns to predict the target word for a given context, and the Skip-Gram model, which, on the contrary, learns to predict the nearby words of a given word. Finally, two different corpora (IMDB and Reuters) are also alternated resulting in 16 different models, which are then ready to be tested and evaluated." 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "- corpus: IMDB and Reuters\n",
    "- window size: 3, 5, 7, 9\n",
    "- architecture: CBOW and skip-gram\n",
    "\n",
    "**Worth considering:**\n",
    "\n",
    "- different starting values for weights (-alpha in Mikolov w2v)\n",
    "- different amount of iterations/epochs\n",
    "\n",
    "Models should be saved into the `models/` directory. Naming should be model-corpus-windowsize-arch, e.g. `model-imdb-5-skip`.\n",
    "\n",
    "Example usage of Mikolov's word2vec for our case (you need to modify filepaths for it to work):\n",
    "\n",
    "`./word2vec -train reuters_51cls.json.tokenized -output model-reuters-3-cbow -window 3 -cbow 1`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
